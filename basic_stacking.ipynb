{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_stacking.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "mUKhe-vC-LIw",
        "445ZsYCCGY-3",
        "QPypOQs6GY_Q",
        "uCWZkCB2r79M",
        "PiR0T0qiupmX",
        "DXx8D-PE6gnU",
        "OEp4sO4pqGPC",
        "ko7UpPFwGY_f",
        "oIFHkOA5GY_1",
        "XLFcWPbUGZAe",
        "a8yNlwH1GZBL",
        "noWn7uBNGZBh",
        "Oiw32ImTGZB_",
        "cKfBoGm8R4q-"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstroBoy1/santander/blob/master/basic_stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mUKhe-vC-LIw"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "***"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "dcf9c1f5-27a2-48b4-fc8c-e9a8cbc3f366",
        "id": "3JM3uflR-LIi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 uninstall xgboost\n",
        "!pip3 install xgboost\n",
        "!pip uninstall sklearn\n",
        "!pip install sklearn\n",
        "!pip install scikit-optimize\n",
        "!pip install vecstack\n",
        "!pip install catboost"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling xgboost-0.82:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/xgboost-0.82.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/xgboost/*\n",
            "    /usr/local/xgboost/libxgboost.so\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled xgboost-0.82\n",
            "Collecting xgboost\n",
            "  Using cached https://files.pythonhosted.org/packages/6a/49/7e10686647f741bd9c8918b0decdb94135b542fe372ca1100739b8529503/xgboost-0.82-py2.py3-none-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.1.0)\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-0.82\n",
            "Uninstalling sklearn-0.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/sklearn-0.0.dist-info/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled sklearn-0.0\n",
            "Collecting sklearn\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.14.6)\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.14.6)\n",
            "Requirement already satisfied: vecstack in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from vecstack) (1.14.6)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from vecstack) (0.20.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from vecstack) (1.1.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.13.1)\n",
            "Requirement already satisfied: enum34 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.6)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.11.0)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.22.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uC48gUBv-LIX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a5ed036a-7c94-403c-8ace-49c5357c0874"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import time\n",
        "import statistics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from vecstack import stacking\n",
        "from vecstack import StackingTransformer\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import re \n",
        "np.random.seed(0) # ensure reproducibility\n",
        "np.set_printoptions(suppress = True)\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import log_loss\n",
        "# Models\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# Stacking\n",
        "from vecstack import stacking\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization, GaussianNoise\n",
        "from keras import callbacks\n",
        "import keras.backend as K\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "#from skopt import BayesSearchCV"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "95473742-1271-49a1-af36-68711b69f919",
        "id": "IETJszVE-LIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O0bwvFih-LHi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_fn = '/content/gdrive/My Drive/santander_data/train.csv'\n",
        "valid_fn = '/content/gdrive/My Drive/santander_data/test.csv'\n",
        "pred_fn = '/content/gdrive/My Drive/santander_data/submission12.csv'\n",
        "train_data_df = pd.read_csv(train_fn)\n",
        "test_data_df = pd.read_csv(valid_fn)\n",
        "train_data_x = train_data_df.drop(columns=[\"ID_code\", \"target\"]).values\n",
        "train_data_y = train_data_df[\"target\"].values\n",
        "test_data_x = test_data_df.drop(columns=[\"ID_code\"]).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "445ZsYCCGY-3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# AUC metric"
      ]
    },
    {
      "metadata": {
        "id": "FJXuq_R_GyYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def auc(y_true, y_pred):\n",
        "    \"\"\"ROC AUC metric for both binary and multiclass classification.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : 1d numpy array\n",
        "        True class labels\n",
        "    y_pred : 2d numpy array\n",
        "        Predicted probabilities for each class\n",
        "    \"\"\"\n",
        "    ohe = OneHotEncoder(sparse=False)\n",
        "    y_true = ohe.fit_transform(y_true.reshape(-1, 1))\n",
        "    auc_score = roc_auc_score(y_true, y_pred)\n",
        "    return auc_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPypOQs6GY_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ]
    },
    {
      "metadata": {
        "id": "EXs3lkQhGY_U",
        "colab_type": "code",
        "outputId": "0dccb2dd-5196-4c26-c8a2-ef2c11ac7500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# n_classes = 3\n",
        "n_classes = 2\n",
        "# Create data: 500 example, 5 feature, 3 classes\n",
        "# X, y = make_classification(n_samples=500, n_features=5, \n",
        "#                            n_informative=3, n_redundant=1, \n",
        "#                            n_classes=n_classes, flip_y=0, \n",
        "#                            random_state=0)\n",
        "length = 10000\n",
        "X, y = train_data_x[:length], train_data_y[:length]\n",
        "\n",
        "# Make train/test split\n",
        "# As usual in machine learning task we have X_train, y_train, and X_test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "print('Train shape:', X_train.shape)\n",
        "print('Test shape: ', X_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (8000, 200)\n",
            "Test shape:  (2000, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uCWZkCB2r79M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LGB params"
      ]
    },
    {
      "metadata": {
        "id": "7J2mCU9Orz0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lgb_params = {\n",
        "    'bagging_freq': 5,\n",
        "    'bagging_fraction': 0.335,\n",
        "    'boost_from_average':'false',\n",
        "    'boost': 'gbdt',\n",
        "    'feature_fraction': 0.041,\n",
        "    'learning_rate': 0.0083,\n",
        "    'max_depth': -1,\n",
        "    'metric':'auc',\n",
        "    'min_data_in_leaf': 80,\n",
        "    'min_sum_hessian_in_leaf': 10.0,\n",
        "    'num_leaves': 13,\n",
        "    'num_threads': 8,\n",
        "    'tree_learner': 'serial',\n",
        "    'objective': 'binary', \n",
        "    'verbosity': -1,\n",
        "    'categories':'auto'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PiR0T0qiupmX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# XGB params"
      ]
    },
    {
      "metadata": {
        "id": "8aumiZa5urjb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xgb_params = {'tree_method': 'hist',\n",
        " 'objective': 'binary:logistic',\n",
        " 'eval_metric': 'auc',\n",
        " 'learning_rate': 0.0936165921314771,\n",
        " 'max_depth': 2,\n",
        " 'colsample_bytree': 0.3561271102144279,\n",
        " 'subsample': 0.8246604621518232,\n",
        " 'min_child_weight': 53,\n",
        " 'gamma': 9.943467991283027,\n",
        " 'silent': 1,\n",
        " 'categories':'auto'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DXx8D-PE6gnU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Catboost"
      ]
    },
    {
      "metadata": {
        "id": "S6FxgQVJ6fo5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "catboost_params = {'subsample':0.36, #rawdata 0.5  ×2 0.45 ×3 0.36\n",
        "                            #'custom_loss':'Logloss',\n",
        "                            'loss_function':'Logloss',\n",
        "                           'random_strength':0,\n",
        "                           'max_depth':3,\n",
        "                           'eval_metric':\"AUC\",\n",
        "                           'learning_rate':0.02,\n",
        "                           #'iterations':60000,\n",
        "                           'iterations':100,\n",
        "                           #class_weights=[1,2],\n",
        "                           'bootstrap_type':'Bernoulli',\n",
        "                           #rsm=0.045,\n",
        "                            'l2_leaf_reg':0.3,\n",
        "                           #'task_type':\"GPU\",\n",
        "                           'random_seed':432013,\n",
        "                           'od_type':\"Iter\",\n",
        "                           'border_count':128\n",
        "                           #has_time= True \n",
        "                  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OEp4sO4pqGPC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NN"
      ]
    },
    {
      "metadata": {
        "id": "754EyMecqRGj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LOGGER\n",
        "class Logger(callbacks.Callback):\n",
        "    def __init__(self, out_path='./', patience=10, lr_patience=3, out_fn='', log_fn=''):\n",
        "        self.auc = 0\n",
        "        self.path = out_path\n",
        "        self.fn = out_fn\n",
        "        self.patience = patience\n",
        "        self.lr_patience = lr_patience\n",
        "        self.no_improve = 0\n",
        "        self.no_improve_lr = 0\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_train_end(self, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_begin(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        return\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        cv_pred = self.model.predict(self.validation_data[0], batch_size=1024)\n",
        "        cv_true = self.validation_data[1]\n",
        "        auc_val = roc_auc_score(cv_true, cv_pred)\n",
        "        if self.auc < auc_val:\n",
        "            self.no_improve = 0\n",
        "            self.no_improve_lr = 0\n",
        "            print(\"Epoch %s - best AUC: %s\" % (epoch, round(auc_val, 4)))\n",
        "            self.auc = auc_val\n",
        "            self.model.save(self.path + self.fn, overwrite=True)\n",
        "        else:\n",
        "            self.no_improve += 1\n",
        "            self.no_improve_lr += 1\n",
        "            print(\"Epoch %s - current AUC: %s\" % (epoch, round(auc_val, 4)))\n",
        "            if self.no_improve >= self.patience:\n",
        "                self.model.stop_training = True\n",
        "            if self.no_improve_lr >= self.lr_patience:\n",
        "                lr = float(K.get_value(self.model.optimizer.lr))\n",
        "                K.set_value(self.model.optimizer.lr, 0.75*lr)\n",
        "                print(\"Setting lr to {}\".format(0.75*lr))\n",
        "                self.no_improve_lr = 0\n",
        "\n",
        "        return\n",
        "\n",
        "# MODEL DEF\n",
        "def dnn():\n",
        "#     inp = Input(shape=(200, 1))\n",
        "#     d1 = Dense(16, activation='relu')(inp)\n",
        "#     fl = Flatten()(d1)\n",
        "#     preds = Dense(1, activation='sigmoid')(fl)\n",
        "#     model = Model(inputs=inp, outputs=preds)\n",
        "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(16, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "logger = Logger(patience=10, out_path=save_directory, out_fn='cv_{}.h5')\n",
        "#nn_params = {'nb_epoch':32, 'batch_size':256, 'callbacks':[logger], 'verbose':1}\n",
        "nn_params = {'epochs':32, 'batch_size':256, 'verbose':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ko7UpPFwGY_f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize 1st level models"
      ]
    },
    {
      "metadata": {
        "id": "yAaA1t3IGY_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_keras_model_1():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, \n",
        "                    input_dim=X_train.shape[1], \n",
        "                    kernel_initializer='normal', \n",
        "                    activation='relu'))\n",
        "    model.add(Dense(n_classes, \n",
        "                    kernel_initializer='normal', \n",
        "                    activation='softmax'))\n",
        "    model.compile(optimizer='rmsprop', \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['categorical_accuracy'])\n",
        "    return model\n",
        "# Caution! All models and parameter values are just \n",
        "# demonstrational and shouldn't be considered as recommended.\n",
        "models_1 = [ \n",
        "    # GaussianNB(),\n",
        "    \n",
        "    #LogisticRegression(random_state=0),\n",
        "    \n",
        "    #ExtraTreesClassifier(random_state=0, n_jobs=-1, \n",
        "    #                     n_estimators=100, max_depth=3),\n",
        "                         \n",
        "    #RandomForestClassifier(random_state=0, n_jobs=-1, \n",
        "    #                       n_estimators=100, max_depth=3),\n",
        "        \n",
        "    #XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "    #              n_estimators=100, max_depth=3, categories='auto'),\n",
        "    XGBClassifier(**xgb_params),           \n",
        "    #LGBMClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "    #               n_estimators=100, max_depth=3)#,\n",
        "    LGBMClassifier(**lgb_params),\n",
        "    CatBoostClassifier(**catboost_params),\n",
        "    #model.fit(X_train, y_train_, validation_data=(X_valid, y_valid))\n",
        "    #Pipeline([('sc', StandardScaler()), ('clf', KerasClassifier(build_fn=dnn, **nn_params))])\n",
        "    #Pipeline([('sc', StandardScaler()), ('clf', Ridge(**ridge_params))])\n",
        "    #KerasClassifier(build_fn=build_keras_model_1, epochs=2, \n",
        "    #                batch_size=32, verbose=0)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "onNywV8MGY_o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Perform stacking"
      ]
    },
    {
      "metadata": {
        "id": "pfceZw1BGY_q",
        "colab_type": "code",
        "outputId": "dc8c2833-8f7d-4bfc-dd02-733a2224d5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2143
        }
      },
      "cell_type": "code",
      "source": [
        "save_directory = '/content/gdrive/My Drive/santander_results/'\n",
        "S_train_1, S_test_1 = stacking(models_1,                   # list of models\n",
        "                               X_train, y_train, test_data_x,   # data\n",
        "                               regression=False,           # classification task (if you need \n",
        "                                                           #     regression - set to True)\n",
        "                               mode='oof_pred',            # mode: oof for train set, fit on full \n",
        "                                                           #     train and predict test set once\n",
        "                               needs_proba=True,           # predict probabilities (if you need \n",
        "                                                           #     class labels - set to False) \n",
        "                               ##save_dir='.',               # save result and log in current dir \n",
        "                               save_dir=save_directory,                                 #     (to disable saving - set to None)\n",
        "                               metric=auc,            # metric: callable\n",
        "                               n_folds=5,                  # number of folds\n",
        "                               stratified=True,            # stratified split for folds\n",
        "                               shuffle=True,               # shuffle the data\n",
        "                               random_state=0,             # ensure reproducibility\n",
        "                               verbose=1)                  # print all info\n",
        "# S_train_1, S_test_1 = stacking(models_1,                   # list of models\n",
        "#                                X_train, y_train, X_test,   # data\n",
        "#                                regression=False,           # classification task (if you need \n",
        "#                                                            #     regression - set to True)\n",
        "#                                mode='oof_pred',            # mode: oof for train set, fit on full \n",
        "#                                                            #     train and predict test set once\n",
        "#                                needs_proba=True,           # predict probabilities (if you need \n",
        "#                                                            #     class labels - set to False) \n",
        "#                                ##save_dir='.',               # save result and log in current dir \n",
        "#                                save_dir=save_directory,                                 #     (to disable saving - set to None)\n",
        "#                                metric=auc,            # metric: callable\n",
        "#                                n_folds=5,                  # number of folds\n",
        "#                                stratified=True,            # stratified split for folds\n",
        "#                                shuffle=True,               # shuffle the data\n",
        "#                                random_state=0,             # ensure reproducibility\n",
        "#                                verbose=1)                  # print all info"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task:         [classification]\n",
            "n_classes:    [2]\n",
            "metric:       [auc]\n",
            "mode:         [oof_pred]\n",
            "n_models:     [3]\n",
            "\n",
            "model  0:     [XGBClassifier]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    ----\n",
            "    MEAN:     [0.76285922] + [0.01258197]\n",
            "    FULL:     [0.76310255]\n",
            "\n",
            "    Fitting on full train set...\n",
            "\n",
            "model  1:     [LGBMClassifier]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    ----\n",
            "    MEAN:     [0.70556046] + [0.01059175]\n",
            "    FULL:     [0.70436275]\n",
            "\n",
            "    Fitting on full train set...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d1a8167e8736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# shuffle the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# ensure reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                verbose=1)                  # print all info\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/vecstack/core.py\u001b[0m in \u001b[0;36mstacking\u001b[0;34m(models, X_train, y_train, X_test, sample_weight, regression, transform_target, transform_pred, mode, needs_proba, save_dir, metric, n_folds, stratified, shuffle, random_state, verbose)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    Fitting on full train set...\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'predict_proba'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0mcol_slice_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_counter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_counter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/vecstack/core.py\u001b[0m in \u001b[0;36mmodel_action\u001b[0;34m(model, X_train, y_train, X_test, sample_weight, action, transform)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# 'sample_weight' parameter of fit method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMCheckXY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0m_LGBMCheckConsistentLength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [8000, 8000, 40000000]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oIFHkOA5GY_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Look at the result"
      ]
    },
    {
      "metadata": {
        "id": "_mF4qZYMGY_5",
        "colab_type": "code",
        "outputId": "12696d42-e7a6-4489-e8e7-2d9db3f9be0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "print('We have %d classes and %d models so in resulting arrays \\\n",
        "we expect to see %d columns.' % (n_classes, len(models_1), n_classes * len(models_1)))\n",
        "print('S_train_1 shape:', S_train_1.shape)\n",
        "print('S_test_1 shape: ', S_test_1.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 2 classes and 6 models so in resulting arrays we expect to see 12 columns.\n",
            "S_train_1 shape: (8000, 12)\n",
            "S_test_1 shape:  (2000, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "1Pycd3ftGZAG",
        "colab_type": "code",
        "outputId": "055382b5-c4f4-4bff-fae0-d34eea48c0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "S_train_1[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.88595608, 0.11404392, 0.87512032, 0.12487968, 0.90076603,\n",
              "        0.09923397, 0.91364649, 0.08635351, 0.86732459, 0.13267542,\n",
              "        0.91284642, 0.08715358],\n",
              "       [0.99596026, 0.00403974, 0.98994982, 0.01005018, 0.90782715,\n",
              "        0.09217285, 0.91324549, 0.08675451, 0.96264553, 0.03735447,\n",
              "        0.96822305, 0.03177695]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "MWVUO6kUGZAX",
        "colab_type": "code",
        "outputId": "79c5b606-4dfd-4b30-f66c-23285136e915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "S_test_1[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.96200187, 0.03799813, 0.94323898, 0.05676102, 0.89576226,\n",
              "        0.10423774, 0.89789474, 0.10210526, 0.92684668, 0.07315334,\n",
              "        0.93933053, 0.06066947],\n",
              "       [0.99292921, 0.00707079, 0.9939683 , 0.0060317 , 0.90598083,\n",
              "        0.09401917, 0.90470871, 0.09529129, 0.95539749, 0.04460249,\n",
              "        0.95137477, 0.04862523]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "NGHPNRwWbLcb",
        "colab_type": "code",
        "outputId": "4dc2d31f-4e7d-4420-ae61-ae9b938b5b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "S_train_1[0][0], S_train_1[0][1], S_train_1[0][1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8859560775904103, 0.11404392240959342, 0.11404392240959342)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "XLFcWPbUGZAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Our arrays and log were saved in current dir"
      ]
    },
    {
      "metadata": {
        "id": "LYLFLXtdGZAi",
        "colab_type": "code",
        "outputId": "5d1c539a-3ecf-4c8e-f5b7-de7a44e746a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "names = sorted(glob(save_directory + '*.npy'))\n",
        "npy_1_name = names[0] # for later use\n",
        "\n",
        "print('Arrays:')\n",
        "for name in names:\n",
        "    print(name)\n",
        "\n",
        "names = sorted(glob(save_directory + '*.log.txt'))\n",
        "log_1_name = names[0] # for later use\n",
        "\n",
        "print('\\nLogs:')\n",
        "for name in names:\n",
        "    print(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arrays:\n",
            "/content/gdrive/My Drive/santander_results/[2019.03.26].[04.42.41].477166.c4aa2f.npy\n",
            "\n",
            "Logs:\n",
            "/content/gdrive/My Drive/santander_results/[2019.03.26].[04.42.41].477166.c4aa2f.log.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LQcidKspGZBK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Time to collect results\n",
        "\n",
        "After several (many) days of building, optimizing, and testing models we have a lot of files with saved OOF.  \n",
        "At this point we can load and use OOF of specific model or all OOF we have."
      ]
    },
    {
      "metadata": {
        "id": "a8yNlwH1GZBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Find specific model\n",
        "\n",
        "We can open logs and find the model of interest.  \n",
        "We can do it programmatically or just open logs in editor.  \n",
        "Name of the `.log.txt` file is the same as the name of corresponding `.npy` file (except extension).  \n",
        "To find columns containing OOF of specific model we use model index from log:\n",
        "* if we predicted class labels - corresponding column index is just model index\n",
        "* if we predicted probabilities - corresponding column index is model index multiplied by number of classes"
      ]
    },
    {
      "metadata": {
        "id": "IhFQcWkbGZBR",
        "colab_type": "code",
        "outputId": "a548740a-f93e-45d5-d96f-96d51e5b6670",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Let's open this log: %s\" % log_1_name)\n",
        "with open(log_1_name) as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "print(\"Let's look what models did we build in those session.\\n\")\n",
        "for line in lines:\n",
        "    if re.search(r'^model [0-9]+', line):\n",
        "        print(line)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's open this log: [2018.02.01].[15.41.41].305268.0eadc0.log.txt\n",
            "Let's look what models did we build in those session.\n",
            "\n",
            "model 0:    [GaussianNB]\n",
            "\n",
            "model 1:    [LogisticRegression]\n",
            "\n",
            "model 2:    [ExtraTreesClassifier]\n",
            "\n",
            "model 3:    [RandomForestClassifier]\n",
            "\n",
            "model 4:    [XGBClassifier]\n",
            "\n",
            "model 5:    [LGBMClassifier]\n",
            "\n",
            "model 6:    [KerasClassifier]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "noWn7uBNGZBh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load specific model OOF\n",
        "\n",
        "Let's say we are interested in `LGBMClassifier`.  \n",
        "We found out that it has index 5.  \n",
        "Then we load target `.npy` file and because of probabilities we need 3 columns from 15 (5 \\* 3) to 18 (5 \\* 3 + 3)"
      ]
    },
    {
      "metadata": {
        "id": "poHXaLPOGZBj",
        "colab_type": "code",
        "outputId": "4b7ee7d1-140e-4f54-bc71-7c50e0d214e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Let's load this .npy file: %s\" % npy_1_name)\n",
        "S = np.load(npy_1_name)\n",
        "S_train_lgbm = S[0][:, 15:18]\n",
        "S_test_lgbm = S[1][:, 15:18]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let's load this .npy file: /content/gdrive/My Drive/santander_results/[2019.03.26].[04.42.41].477166.c4aa2f.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WK5YNb_fGZBu",
        "colab_type": "code",
        "outputId": "4346f7fb-9f58-445a-dffd-f1a4a8f2cf3e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "S_train_lgbm[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00040829,  0.00281319,  0.99677852],\n",
              "       [ 0.99732125,  0.00258249,  0.00009626],\n",
              "       [ 0.98322854,  0.01610955,  0.00066191],\n",
              "       [ 0.00107737,  0.99633895,  0.00258368],\n",
              "       [ 0.97101719,  0.02843959,  0.00054321]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "f5soz3_LGZB5",
        "colab_type": "code",
        "outputId": "576b3e46-6653-491b-cd48-a5d84ddc3544",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "S_test_lgbm[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.60639131,  0.3588515 ,  0.03475718],\n",
              "       [ 0.03609523,  0.90174785,  0.06215692],\n",
              "       [ 0.08650007,  0.89717473,  0.0163252 ],\n",
              "       [ 0.00068572,  0.98858075,  0.01073353],\n",
              "       [ 0.00122693,  0.99814513,  0.00062793]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Oiw32ImTGZB_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Compute score of specific model"
      ]
    },
    {
      "metadata": {
        "id": "JtGm0YvLGZCA",
        "colab_type": "code",
        "outputId": "7df706b5-39ec-4c29-8394-160b634ce124",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('LGBMCLassifier log loss: %.8f' % log_loss(y_train, S_train_lgbm))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LGBMCLassifier log loss: 0.41430248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "72UtXfCTGZCG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load ALL OOF\n",
        "\n",
        "***Note:*** If you load OOF from scratch, don't forget to load `y_train` from initial dataset too."
      ]
    },
    {
      "metadata": {
        "id": "JDrtmfPoGZCP",
        "colab_type": "code",
        "outputId": "f9030967-6cbc-4981-ef41-e1beed8786f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "cell_type": "code",
      "source": [
        "# Create empty arrays\n",
        "S_train_all = np.zeros((X_train.shape[0], 0))\n",
        "S_test_all = np.zeros((X_test.shape[0], 0))\n",
        "\n",
        "# Load results\n",
        "for name in sorted(glob(save_directory + '*.npy')):\n",
        "    print('Loading: %s' % name)\n",
        "    S = np.load(name)\n",
        "    S_train_all = np.c_[S_train_all, S[0]]\n",
        "    S_test_all = np.c_[S_test_all, S[1]]\n",
        "    \n",
        "print('\\nS_train_all shape:', S_train_all.shape)\n",
        "print('S_test_all shape: ', S_test_all.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading: /content/gdrive/My Drive/santander_results/[2019.03.27].[02.18.45].735709.7f0aac.npy\n",
            "Loading: /content/gdrive/My Drive/santander_results/[2019.03.27].[02.26.35].716784.c4aa2f.npy\n",
            "\n",
            "S_train_all shape: (8000, 14)\n",
            "S_test_all shape:  (2000, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fpp3rRIjGZCb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Apply 2nd level model"
      ]
    },
    {
      "metadata": {
        "id": "hBK3gZsdexEw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just use class 0 probability"
      ]
    },
    {
      "metadata": {
        "id": "7Bw6tBH9eMLL",
        "colab_type": "code",
        "outputId": "28891c73-d531-4f92-b379-890d88ab9a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "columns = [n for n in range(0, S_train_all.shape[1], 2)]\n",
        "l2_train = S_train_all[:, columns]\n",
        "l2_test = S_test_all[:, columns]\n",
        "l2_train.shape, l2_test.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8000, 7), (2000, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "qtxiOpwGGZCb",
        "colab_type": "code",
        "outputId": "ded7c110-7437-4d24-bb10-501f277b4e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize 2nd level model\n",
        "model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "                      n_estimators=100, max_depth=3)\n",
        "#model = LogisticRegression(random_state=0)\n",
        "    \n",
        "\n",
        "# Fit 2nd level model\n",
        "# model = model.fit(S_train_all, y_train)\n",
        "model = model.fit(l2_train, y_train)\n",
        "\n",
        "\n",
        "# Predict\n",
        "# y_pred = model.predict_proba(S_test_all)\n",
        "y_pred = model.predict_proba(l2_test)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Final prediction score\n",
        "# print('Final prediction score: %.8f' % log_loss(y_test, y_pred))\n",
        "y_pred_final = [elem[1] for elem in y_pred]\n",
        "roc_auc_score(y_test, y_pred_final)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8552902565006222"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "VB2dW9VH9fCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "7ffbec7a-011d-4355-fdf2-106d4b8fd020"
      },
      "cell_type": "code",
      "source": [
        "# Initialize 2nd level model\n",
        "#model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "#                      n_estimators=100, max_depth=3)\n",
        "model = LogisticRegression(random_state=0)\n",
        "    \n",
        "\n",
        "# Fit 2nd level model\n",
        "# model = model.fit(S_train_all, y_train)\n",
        "model = model.fit(l2_train, y_train)\n",
        "\n",
        "\n",
        "# Predict\n",
        "# y_pred = model.predict_proba(S_test_all)\n",
        "y_pred = model.predict_proba(l2_test)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Final prediction score\n",
        "# print('Final prediction score: %.8f' % log_loss(y_test, y_pred))\n",
        "y_pred_final = [elem[1] for elem in y_pred]\n",
        "roc_auc_score(y_test, y_pred_final)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8527737089615524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "LRrXb5cMfOkn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No change with redundant features"
      ]
    },
    {
      "metadata": {
        "id": "cKfBoGm8R4q-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# redundant features test"
      ]
    },
    {
      "metadata": {
        "id": "oC2kEuUZeFrC",
        "colab_type": "code",
        "outputId": "40006c65-328a-4070-c3bf-48eeeca933da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize 2nd level model\n",
        "model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "                      n_estimators=100, max_depth=3)\n",
        "    \n",
        "\n",
        "# Fit 2nd level model\n",
        "model = model.fit(S_train_all, y_train)\n",
        "#model = model.fit(l2_train, y_train)\n",
        "\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict_proba(S_test_all)\n",
        "#y_pred = model.predict_proba(l2_test)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Final prediction score\n",
        "# print('Final prediction score: %.8f' % log_loss(y_test, y_pred))\n",
        "y_pred_final = [elem[1] for elem in y_pred]\n",
        "roc_auc_score(y_test, y_pred_final)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8461037566493023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "A4_abMa7R6o_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lPVuCOj-sQn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predictions on the test set"
      ]
    },
    {
      "metadata": {
        "id": "zaXY2Si0-v4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize 2nd level model\n",
        "#model = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n",
        "#                      n_estimators=100, max_depth=3)\n",
        "model = LogisticRegression(random_state=0)\n",
        "    \n",
        "\n",
        "# Fit 2nd level model\n",
        "# model = model.fit(S_train_all, y_train)\n",
        "model = model.fit(l2_train, y_train)\n",
        "\n",
        "\n",
        "# Predict\n",
        "# y_pred = model.predict_proba(S_test_all)\n",
        "y_pred = model.predict_proba(l2_test)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Final prediction score\n",
        "# print('Final prediction score: %.8f' % log_loss(y_test, y_pred))\n",
        "y_pred_final = [elem[1] for elem in y_pred]\n",
        "# roc_auc_score(y_test, y_pred_final)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}